---
title: "Predicting Ductal Invasive Carcinomas via CNN"
author: "Eduardo Coronado"
date: "5/22/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
suppressMessages(library(tidyverse))
library(tensorflow)
library(keras)
library(here)
library(curl)
library(knitr)
library(png)
library(caret)
suppressMessages(library(filesstrings))
suppressMessages(source("./extract_carcinoma_data.R"))
set.seed(1)
```

**_Example and code based on:_**
 
* Baruah, Bikram, and Bikram Baruah. “Predicting Invasive Ductal Carcinoma Using Convolutional Neural Network (CNN) in Keras.” Towards Data Science, Towards Data Science, 3 Jan. 2019, towardsdatascience.com/predicting-invasive-ductal-carcinoma-using-convolutional-neural-network-cnn-in-keras-debb429de9a6. 

* Choosehappy. “Use Case 6: Invasive Ductal Carcinoma (IDC) Segmentation.” Andrew Janowczyk, 5 Jan. 2018, www.andrewjanowczyk.com/use-case-6-invasive-ductal-carcinoma-idc-segmentation/. 
  
#### **Before starting the exercise**  
If you'd like to quickly knit an example using pre-computed data please download and unzip the files found in [**this link**](https://duke.box.com/s/4ouru5cu3b9sg7bn375st7m2nhfrkdyt).  
  
Otherwise, uncomment the line with the `extract_carcinoma_data` helper function.  
This will take a couple of hour to run but will generate both the data and cache files for easy tuning of the model if needed.  
  
### 1. Dataset  
The original data set consists of 162 whole mount slide images from breast cancer samples that were scanned at 40x. However, since this whole mounts cannot be easily processed or analyzed without some pre-processing they were segmented into 50px x 50px patches to generate almost 300,000 image files (1.6 Gb) - 60\% classified as benign and 40\% as malignant.  

Given image files are large and require memory storage, we will focus on exploring a subset. This was done using a custom function `extract_carcinoma_data` which downloads the 1.6 Gb patches file, randomly selects `n` images and stores these in a temporary directory `data_idc`. The number of files to select can be specified using the `files_to_keep` variable, **but it is not recommended you select more than 100,000** as this will require to load around 2 Gb of data onto your RAM memory. Additionally, the helper function conducts some environment cleaning routines to remove unnecessary variables.


```{r}
# Check if data directories exists in current repo, if not
# download for original data file and pre process images
# for TRAIN and TEST folders
subdir <- "invasive-carcinoma"
data_dir <- file.path(here(), subdir, "data_idc")
train_dir <- file.path(here(), subdir, "TRAIN")
test_dir <- file.path(here(), subdir, "TEST")
files_to_keep <- 35000

# Helper function that downloads original data,
# extracts necessary data and stores it in respective folder

# extract_carcinoma_data(subdir, train_dir, test_dir, files_to_keep )

```

### 2. Pre-processing  
  
After the files to analyze are selected some further processing must happen given some patches are not the correct dimensions needed for analysis - 50x50x3 - where 50x50 is the pixel dimensions and 3 refers to a RGB color spectrum (e.g. for black and white you'd see a 50x50x1).  

This task is done via the `process_images` custom function. It first lists the files in `data_idc`, splits them into malignant vs benign based on the filename terminations (i.e. "...class0"" for benign or "...class1" for malignant). Then the images are read one at a time, reshaped into 50x50x3 arrays with normalized pixel values between 0 and 1. Afterward it stores the image classification as `0` or `1` depending on the whether the processed image was benign or malignant, respectively.


```{r}
# Get file paths of all image patches saves as PNGs in data directory
image_patches <- list.files(data_dir, pattern = "*.png", recursive = TRUE,
                             full.names = TRUE)

#From all files, create subsets of those that are classified as malignant(1),
# or benign(0)
class_zero <- list.files(data_dir, pattern = "*class0.png", recursive = TRUE, 
                         full.names = TRUE)
class_one  <-  list.files(data_dir, pattern = "*class1.png", recursive = TRUE, 
                         full.names = TRUE)

# Helper function to preprocess images, not all in original zip 
# are 50x50 color images (50,50,3)
process_images <-  function(lower_idx, upper_idx){
  
  img_width = img_height <- 50 # image width and height
  channels <- 3 # 3 channels = color, 1 channel = BW
  p <- progress_estimated(upper_idx) # set progress bar
  
  # Pre-allocate space for efficiency
  X <- array(NA, dim= c(upper_idx, img_width, img_height, channels))
  y <- rep(NA, upper_idx)
  
  # Loop over all PNG files and load them as normalized (50, 50, 3) matrices
  for (i in lower_idx:upper_idx){
    # Store images as matrices and normalize to get values between 0-1
    X[i,,,] <- image_load(image_patches[i], target_size = c(img_width, img_height), 
                     interpolation = "bicubic") %>% 
    image_to_array(.) * (1/255)
    
    # Now, if image is malignant or benign store its classification accordintly
    if(image_patches[i] %in% class_zero){
      y[i] <- "0"
    } else {
      y[i] <- "1"
    }
    
    p$tick()$print()
  }
  
  return(list(X,y)) # return X,y objects
  
}
```


```{r img_preproc, cache=TRUE}
# Run helper function for all images data directory,
# however inputs allow to pre-process subsets via a
# lower and upper index bounds
x_y <- process_images(1,length(image_patches))

```

### 3. Class balancing  
  
Now that we have all the data we must check for class imbalance (i.e. if the number of 0s and 1s are the same or not). This is important to reduce model bias since data imbalance can cause the model to incorrectly classify the minority class and the majority class (i.e. classify malignant as benign).  
  
From the table below we can observe there are more benign cases in our subset. We will address this later using an R package.

```{r output_preproc}
# Extract preprocessing output
X <- x_y[[1]]
Y <- x_y[[2]]

# Create table to show num of classes stored and 
# preprocessed - NOTE: this shows class imbalance
kable(table(Y) %>% t()) 
```

First, we split our subset into training and test data (85\% and 15\% respectively).  

```{r train_test_split}
# Train / Test split
samp_size <- dim(X)[1] # number of files preprocesses (e.g. 100,000)

# 15-85 test/train split indexes
train <- sample(seq_len(samp_size), size = 0.85 * samp_size) 

# Store train and test splits for both class and features
X_train <- X[train,,,]
X_test  <- X[-train,,,]
y_train <- Y[train]
y_test  <- Y[-train]

# Clean-up and save space
rm(X, Y, x_y, class_one,
   class_zero)

```

Now to tackle the class imbalance we will use the `caret` R-package. However, the function we will use doesn't take in an multidimensional array (e.g. 50x50x3) and instead takes an `n`x`m` table/matrix. Therefore, we must flatten the arrays into 1 x `m` rows and stack them together.  

```{r, warning=FALSE}
# First reshape matrices to feed into function to
# tackle imbalance
nobs_train <- dim(X_train)[1]
nobs_test  <- dim(X_test)[1]

# Shape of new rows will be (1, 50*50*3) = (1, 7500)
X_train_shape <- (dim(X_train)[2]^2) * dim(X_train)[4]
X_test_shape  <- (dim(X_test)[2]^2) * dim(X_test)[4]

# For each array, reshape it as (1, 7500) and stack into dataframe
X_train <- array_reshape(X_train, 
                         dim = c(nobs_train, X_train_shape)) %>% as_tibble()
X_test  <- array_reshape(X_test, 
                         dim = c(nobs_test, X_test_shape)) %>%  as_tibble()

```

This process is done for both the train (`train_df`) and test data (`test_df`). In our case we will want to undersample the majority class (i.e. benign). Using the `downSample` function from the `caret` package we're able to tackle the class imbalance issues.   
  
The table below demonstrates the differences between pre- and post- class balancing using the `downSample` function. As one can observe the `downSample` function under samples the majority class and makes the classes balanced (ie. same number of 0s and 1s).

```{r imbalance}
# Tackle imbalance but store pre- and post downsampling to
# show effects of balancing classes

idxs <- seq(1, files_to_keep) # Create vector with 1-100,000

# Create a test dataframe with the patch index (to be used later),
# the class (y), and features
test_df <- bind_cols(idx = idxs[!(idxs %in% train)],
                     y = y_test, X_test)

# Store current class imbalance for test dataset
samp_df <- test_df %>% 
  group_by(y) %>% 
  summarize(test_original = n())

# Use `caret` function to downsample rows such that classes are balanced
# based on the class with less counts
test_df <- downSample(x = test_df[,-2], y = factor(test_df$y)) %>% 
  mutate(y = as.numeric(Class) - 1) %>% 
  select(y, everything(), -Class)

# Append new class balance for test data
samp_df <- bind_cols(samp_df, balanced_test = as.integer(table(test_df$y)))
rm(X_test) # clean-up space


# Do same procedure as above for train data
train_df <- bind_cols(idx = idxs[idxs %in% train],
                      y = factor(y_train), X_train)
samp_df  <- bind_cols(samp_df, train_original = as.integer(table(train_df$y)))

train_df <- downSample(x = train_df[,-2], y = factor(train_df$y)) %>% 
  mutate(y = as.numeric(Class) -1) %>% 
  select(y, everything(), -Class)

samp_df <- bind_cols(samp_df, balanced_train = as.integer(table(train_df$y)))
rm(X_train)

# Show table with pre/post class balancing techniques
kable(samp_df)
```

### 4. Model  
  
After balancing the classes we move to creating the model architecture. Similar to the Xrays example, here we use a sequential model that includes convolutional layers, max pooling layers, drop-out layers and fully connected layers.  

The overall architecture is composed of ReLU activated layers, starting with a 32 neurons layer and kernel size of 3x3, followed by a max pooling layer with a pool size  of 2x2. Then another layer with 64 neurons and same kernel followed by another max pooling layer. Subsequent layers with increasing number of neurons - 128 and 256. Afterward we need flatten the 3D feature map into a 1D feature vectors before including it in the fully connected layers with 128 neurons. In between these fully connected layers there are two drop out layers with a drop out rate of 50\% which means half of the neurons will be turned off at random. The final output layer is a dense layer with `num_classes = 1` since the classes are not one-hot encoded (for one hot encode the num of classes would be 2 for binary outcomes).  

```{r model}
# Create model
batch_size <- 50 # Large batch size due to big # of images
epochs <- 40 
num_classes <- 1 # One-hot encoding, for OHE num_class = 2 (binary)

# Set up sequential model
model <- keras_model_sequential()

# Set convolutional layers with kernel_size of a 3x3 grid, 
# max po0ling of 2x2 and relu activation. Two fully connected layers
# are included with a subsequent dropout rate of 50%
# Output layer has sigmoid activation for binary
# Input shape is (50,50,3) arrays

model %>% layer_conv_2d(filters = 32, kernel_size = c(3,3), 
                        activation = "relu", input_shape = c(50, 50, 3)) %>% 
          layer_max_pooling_2d(pool_size = c(2,2)) %>% 
          layer_conv_2d(64, kernel_size = c(3, 3), activation = "relu") %>% 
          layer_max_pooling_2d(pool_size = c(2, 2)) %>%
          layer_conv_2d(128, kernel_size = c(3, 3), activation = "relu") %>%
          layer_conv_2d(256, kernel_size = c(3, 3), activation = "relu") %>% 
          layer_flatten() %>%
          layer_dropout(rate = 0.5) %>% 
          layer_dense(128, activation = "relu") %>%
          layer_dropout(rate = 0.5) %>% 
          layer_dense(128, activation = "relu") %>%
          layer_dense(num_classes, activation = "sigmoid")


# Output model summary
summary(model)
```

### 5. Compiling the Model  
  
Next we compile the model using the built-in Adam optimizer as before (`optimizer_adam`). There are many other gradient descent optimizers in `keras` but here we will use Adam as an example with commonly used settings. However, we reduced the learning rate (`lr`) to allow for transfer learning as shown below. The type of loss used is `binary_crossentropy` since there are only 2 classes - malignant and benign.  
  
  
```{r compile}
# Compile model with Adama SGD optimizer with a small learning rate
# binarry cross entropy loss and saving accuracy
model %>% compile(optimizer = optimizer_adam(lr = 0.00001),
                  loss = "binary_crossentropy",
                  metrics= "accuracy")


```

### 6. Data Augmentation  
  
Since deep learning tends to work better as we get more data due to the fact that we need to train the neural network's weights. This is especially important in this case since there is **no transfer learning** involved. Using the keras built-in `image_data_generator` we can generate more images with different configurations such as rotations, flips and featurewise normalization/centering.  

```{r data augmentation}
# Data augmentation that includes vertical/horizontal flips
# as well as rotations, and some feature specific normalizations
datagen <- image_data_generator(featurewise_center = TRUE,
                                featurewise_std_normalization = TRUE,
                                rotation_range = 180,
                                horizontal_flip = TRUE,
                                vertical_flip = TRUE
                                )


```


### 7. Model Checkpoints 
Additionally, `keras` allows us to set callbacks to monitor wether the model is learning after each epoch and also store the best model so far so that we don't have to re-run the model training if we were to use it to predict a class in the future.  

```{r callback-fcns}
# Set callbacks functions that are helpful to monitor CNN fit

# Early stop monitoring of validation loss values, if val
# loss doesn't decrease after 3 epochs stop CNN fit
early_stopping_monitor <- callback_early_stopping(monitor = "val_loss",
                                                  patience = 3,
                                                  mode = "min")

# Best model check, if val loss decreases save model as 
# best_model.h5 file in current directory
best_model_path <- file.path(here(), subdir, "best_model.h5")
model_checkpoint <- callback_model_checkpoint(best_model_path,
                                              monitor = "val_loss",
                                              mode = "min",
                                              verbose = 1,
                                              save_best_only = TRUE)

```


### 8. Saving Images for Fit Optimization   
  
Although this step isn't necessary as `keras` can process data from memory, it is an efficient step to optimize the model training when higher throughput computing resources aren't available (e.g. personal laptop). Thus, we save the train and test images into local folders, and clean-up some memory space to avoid a memory over-run.  


```{r save_imgs_2_dir}

# Reshape (n,7500) train/test data frames into NN compatible input shape
# (50, 50, 3)
X_test_reshape <- array_reshape(as.matrix(test_df[,-c(1,2)]),
                                dim = c(nrow(test_df), 50, 50, 3))

X_train_reshape <- array_reshape(as.matrix(train_df[, -(1:2)]),
                                 dim = c(nrow(train_df), 50, 50, 3))



# Helper function to save images to local folders and avoid keeping large
# amounts of data stored in memory and feeding them to CNN
# Inputs: (n, 50,50,3) array, file paths from data directory (image_patches),
# train or test data split indexes saved in df previously (patch_idxs),
# name of directory to create (dir_to_create)
save_images <- function(img_array, image_patches, patch_idxs, dir_to_create){
  
  # Create directory and subdirectories 0 and 1
  img_save_path <- file.path(here(), subdir, dir_to_create)
  dir.create(img_save_path)
  dir.create(file.path(img_save_path, "0"))
  dir.create(file.path(img_save_path, "1"))
  
  p <- progress_estimated(length(patch_idxs)) # Set progress bar
  
  # For each train/test index
  for(i in 1:length(patch_idxs)){
    
    # Get original file path in data directory
    tmp_path <- image_patches[patch_idxs[i]]
    tmp_name <- basename(tmp_path) # Extract filename
    
    # If original file path has 0, store it in 0 subdirectory with original
    # filename extracted (e.g. 10253_idx5_x501_y351_class1.png), else
    # store it in the 1 subdirectory
    if (str_detect(tmp_path, ".*class0.png")){
      writePNG(img_array[i,,,], target = file.path(img_save_path, "0", tmp_name))
    } else {
      writePNG(img_array[i,,,], target = file.path(img_save_path, "1", tmp_name))
    }
    
    p$tick()$print()
  }
  
}

# If TRAIN dir doesn't exist run helper function
if (!dir.exists(file.path(here(), subdir, "TRAIN"))){
  train_idxs <- train_df[,2] # Grab indexes to pass to help fcn
  save_images(X_train_reshape, image_patches, train_idxs, "TRAIN")
}

# If TEST dir doesn't exist run helper function
if (!dir.exists(file.path(here(), subdir, "TEST"))){
  test_idxs <- test_df[,2]  
  save_images(X_test_reshape, image_patches, test_idxs, "TEST")
}


```

### 9. Pre-model train  
  
Prior to training the model we need to specify the directories from which the data augmentation object will take the train and test images, and then feed these to the neural network.   
  

```{r datage_test_train}
# Set up train/test data generator object via datagen
# specified above
# Now files are feed from a directory and not a dataframe/array
# which takes too much memory and can cause computer to slow down
# significantly
train_generator <- flow_images_from_directory(directory = file.path(here(), subdir, "TRAIN"), 
                                              generator = datagen,
                                              target_size = c(50, 50),
                                              batch_size = batch_size,
                                              class_mode = "binary", interpolation = "bicubic",
                                              seed = 2)

test_generator <- flow_images_from_directory(directory = file.path(here(), subdir, "TEST"), 
                                              generator = datagen,
                                              target_size = c(50, 50),
                                              batch_size = batch_size,
                                              class_mode = "binary", interpolation = "bicubic",
                                             seed = 2)

# Clean-up space in memory
rm(X_train_reshape)
rm(X_test_reshape)
```


### 10. Model Fit  
  
Finally, the model is fit using the above train and test data augmentation generators over 40 epochs. This process takes several hours if not Rstudio `cache` file is found in the local folder. Additionally, the model callback functions save the best model to a file called `best_model.h5` which is a `keras` specific file type that can be loaded in any other Rstudio instance without having to re-train a model.  

```{r model_fit, cache=TRUE}

# Fit CNN
hist <- model %>%
  fit_generator(train_generator,
                epochs = epochs,
                steps_per_epoch = as.integer(nrow(train_df)/batch_size),
                validation_data = test_generator,
                validation_steps = as.integer(nrow(test_df)/batch_size),
                verbose = 1,
                callbacks = list(model_checkpoint))

```

### 11. Model Performance and Deep Learning Limitations  
Finally, after running the model we can see that in this case the model doesn't seem to be learning and is overfitting. The overall accuracy seems to be increasing for the training set (max accuracy of approx 75\%) but not for the validation (test) set, which remains roughly at 50\%. This is a clear example of one of the big limitations of deep learning.  

For a neural network to learn sometimes we need to increase the amount of training data or modify the structure of our network. Other possible fixes are potentially increasing the number of epochs, changing the batch size per epoch or increasing/reducing the learning rate. Overall, this is complex problem to solve that is more an art than a precise science - thus it must undergo through a lot of trail and error.  

```{r plot_metrics, warning=FALSE}
#Save accuracy/loss metrics as dataframe
hist_df <- hist$metrics %>% as_tibble()

# Use generic plot function to plot accuracy/loss metrics in ggplot
plot(hist)


```